Namespace(alpha_del=0.4, alpha_sub=0.4, augment_rate=0.15, augment_type='all', batch_size=8, cuda=True, data_path='data', decay=0, epoch=20, freeze_bert=False, gradient_clip=-1, language='ted_talk_with_utt', lr=5e-06, lstm_dim=-1, name='punctuation-restore', pretrained_model='bert-base-uncased', save_path='out/fullTedTalk_newUTT_triangleHalfTo1_weightedLoss_seed1\r', seed=1, sequence_length=256, sliding_window=True, stride_size=0.5, sub_style='unk', trained_model_path=False)
epoch: 0, Train loss: 0.5774779977858951, Train accuracy: 0.8613451522025991
epoch: 0, Val loss: 0.5827393483580768, Val accuracy: 0.864549524509716
val_Precision: [0.98453744 0.5792169  0.62633152 0.49539676 0.59075008]
val_Recall: [0.88935597 0.82371411 0.70967478 0.51223935 0.78163744]
val_F1: [0.93452941 0.68016051 0.66540357 0.50367729 0.67291836]
epoch: 1, Train loss: 0.43885445805764134, Train accuracy: 0.8863178161819825
epoch: 1, Val loss: 0.5514040482138947, Val accuracy: 0.8775580141979625
val_Precision: [0.98489705 0.6067287  0.66195589 0.53297162 0.62039242]
val_Recall: [0.90276196 0.83502267 0.71882253 0.57887579 0.79331736]
val_F1: [0.94204259 0.70280109 0.6892182  0.5549761  0.6962788 ]
epoch: 2, Train loss: 0.4088728617862595, Train accuracy: 0.8934718990607757
epoch: 2, Val loss: 0.5393676812526559, Val accuracy: 0.8833639169321562
val_Precision: [0.98466731 0.64613165 0.61752746 0.49793246 0.63374469]
val_Recall: [0.9099736  0.79363177 0.80448377 0.65503173 0.79442479]
val_F1: [0.9458481  0.7123262  0.69871574 0.56577917 0.70504586]
epoch: 3, Train loss: 0.38987959128076455, Train accuracy: 0.8980443674797719
epoch: 3, Val loss: 0.5247496353254292, Val accuracy: 0.885975975437485
val_Precision: [0.98541605 0.63576027 0.65726335 0.56227328 0.64073399]
val_Recall: [0.91089511 0.83115326 0.75328364 0.63236627 0.80268725]
val_F1: [0.94669133 0.72044359 0.70200529 0.59526349 0.71262496]
epoch: 4, Train loss: 0.3765884256025459, Train accuracy: 0.9008826577276513
epoch: 4, Val loss: 0.5192910323287262, Val accuracy: 0.8918217265089071
val_Precision: [0.9834618  0.6701291  0.63075158 0.57894737 0.65538714]
val_Recall: [0.92080659 0.79260253 0.81079162 0.61332729 0.79494389]
val_F1: [0.95110343 0.72623851 0.70952865 0.59564165 0.71845116]
epoch: 5, Train loss: 0.3650663444109854, Train accuracy: 0.9036558910471197
epoch: 5, Val loss: 0.510732984703104, Val accuracy: 0.8857886882525109
val_Precision: [0.98631643 0.64280585 0.63618769 0.5514158  0.63899884]
val_Recall: [0.91051978 0.80467976 0.80784249 0.67089755 0.80312849]
val_F1: [0.9469037  0.71469154 0.71181262 0.60531697 0.7117238 ]
epoch: 6, Train loss: 0.35521300072568246, Train accuracy: 0.9056593465052147
epoch: 6, Val loss: 0.5162162586762032, Val accuracy: 0.8913415540453036
val_Precision: [0.98524154 0.6579768  0.65118541 0.5483871  0.65376256]
val_Recall: [0.9174182  0.81647037 0.78602441 0.6781505  0.80418401]
val_F1: [0.95012104 0.72870507 0.71227962 0.60640454 0.72121353]
epoch: 7, Train loss: 0.34642474825215247, Train accuracy: 0.9077084531384383
epoch: 7, Val loss: 0.5288599092091821, Val accuracy: 0.889887089736463
val_Precision: [0.98559724 0.65989967 0.63349756 0.58500401 0.64986756]
val_Recall: [0.91543798 0.80210016 0.81810983 0.66137806 0.80448682]
val_F1: [0.94922296 0.7240844  0.7140644  0.62085106 0.71895804]
epoch: 8, Train loss: 0.33774599859322996, Train accuracy: 0.9097028525333765
epoch: 8, Val loss: 0.5191223931362855, Val accuracy: 0.8948581698057194
val_Precision: [0.98457656 0.67435193 0.64635248 0.5641994  0.66320049]
val_Recall: [0.92205167 0.80899213 0.80107042 0.67724388 0.80396771]
val_F1: [0.95228891 0.73556152 0.7154424  0.61557478 0.72683116]
epoch: 9, Train loss: 0.3301155402415432, Train accuracy: 0.9112249098272235
epoch: 9, Val loss: 0.5231856687558393, Val accuracy: 0.8942783764990446
val_Precision: [0.98461015 0.66453292 0.6597123  0.58343434 0.66161321]
val_Recall: [0.92144595 0.81826828 0.78143688 0.65457842 0.80347456]
val_F1: [0.95198146 0.73343104 0.71543394 0.61696219 0.72567582]
epoch: 10, Train loss: 0.321804484286174, Train accuracy: 0.9130000729831763
epoch: 10, Val loss: 0.5339040014757432, Val accuracy: 0.8931287519700022
val_Precision: [0.9852811  0.65251024 0.67698212 0.6010296  0.65868745]
val_Recall: [0.91888072 0.83851425 0.75148139 0.63508613 0.8070564 ]
val_F1: [0.95092318 0.73391033 0.71228906 0.61758872 0.72536265]
epoch: 11, Train loss: 0.31523120050068576, Train accuracy: 0.914507218907105
epoch: 11, Val loss: 0.5390300734074779, Val accuracy: 0.8911482896097453
val_Precision: [0.98588089 0.66455341 0.6337703  0.55673889 0.65229562]
val_Recall: [0.91710758 0.80164417 0.8171814  0.68721668 0.804383  ]
val_F1: [0.95025151 0.72668974 0.71388359 0.61513492 0.72039982]
epoch: 12, Train loss: 0.3082358997054368, Train accuracy: 0.9160577006667859
epoch: 12, Val loss: 0.5409527538414259, Val accuracy: 0.8919372866868698
val_Precision: [0.9856286  0.65989826 0.64803505 0.60195412 0.65517217]
val_Recall: [0.9174648  0.81463338 0.79970509 0.64233908 0.80661516]
val_F1: [0.95032597 0.72914699 0.71592534 0.62149123 0.72304882]
epoch: 13, Train loss: 0.3017926213384437, Train accuracy: 0.9173598196400768
epoch: 13, Val loss: 0.5553721357715815, Val accuracy: 0.8958743024050464
val_Precision: [0.98409246 0.67885244 0.64428899 0.5864962  0.66582566]
val_Recall: [0.92413802 0.80368961 0.80481145 0.66545784 0.80140678]
val_F1: [0.95317339 0.73601508 0.71565937 0.62348694 0.72735196]
epoch: 14, Train loss: 0.2948313879145187, Train accuracy: 0.9186835331954181
epoch: 14, Val loss: 0.5617839618632804, Val accuracy: 0.8909490479236027
val_Precision: [0.98602788 0.64916033 0.66601174 0.56679104 0.65259194]
val_Recall: [0.91575119 0.83046277 0.76827503 0.68857661 0.80805136]
val_F1: [0.94959106 0.72870379 0.71349775 0.6217765  0.72204871]
epoch: 15, Train loss: 0.2878421637691631, Train accuracy: 0.9203680626271927
epoch: 15, Val loss: 0.5842433311572365, Val accuracy: 0.8944796106020486
val_Precision: [0.98460374 0.67265117 0.64226616 0.6016295  0.66149989]
val_Recall: [0.92238041 0.80393715 0.80549412 0.63599275 0.80122509]
val_F1: [0.95247693 0.73245773 0.71467855 0.61833407 0.72468894]
epoch: 16, Train loss: 0.28132659545578353, Train accuracy: 0.9214414258962456
epoch: 16, Val loss: 0.5835980362117714, Val accuracy: 0.8919592032723455
val_Precision: [0.98561927 0.66336707 0.64073272 0.59182848 0.65484534]
val_Recall: [0.91792555 0.81197561 0.79945933 0.6631913  0.80517031]
val_F1: [0.95056875 0.73018681 0.71134922 0.62548097 0.722269  ]
epoch: 17, Train loss: 0.2746541601522938, Train accuracy: 0.9228794583107873
epoch: 17, Val loss: 0.6167262704508473, Val accuracy: 0.8926186932534773
val_Precision: [0.98525781 0.66306125 0.65049092 0.53759398 0.65652816]
val_Recall: [0.91931042 0.81383866 0.78698015 0.7130553  0.80340535]
val_F1: [0.95114237 0.73075348 0.71225564 0.61301637 0.72257844]
epoch: 18, Train loss: 0.26819220853341713, Train accuracy: 0.9242806682281955
epoch: 18, Val loss: 0.6169916511434712, Val accuracy: 0.8948900484755022
val_Precision: [0.98422472 0.67182582 0.6475825  0.60576923 0.6629575 ]
val_Recall: [0.92312849 0.8092527  0.79255072 0.62828649 0.80050699]
val_F1: [0.95269809 0.73416345 0.71277014 0.61682243 0.72526818]
Precision: [0.98409246 0.67885244 0.64428899 0.5864962  0.66582566]
Recall: [0.92413802 0.80368961 0.80481145 0.66545784 0.80140678]
F1 score: [0.95317339 0.73601508 0.71565937 0.62348694 0.72735196]
Accuracy:0.8958743024050464
Confusion Matrix[[357013  22854   6223    230]
 [  5081  61688   9630    357]
 [   627   6073  29473    448]
 [    63    256    419   1468]]
67.88524391720131 80.36896138412632 73.60150811026863 64.42889933326047 80.48114469839709 71.56593740135492 58.64962045545346 66.54578422484134 62.34869399023146 66.58256600464351 80.14067812740629 72.73519642562681

Precision: [0.98675588 0.68473748 0.65355015 0.65804651 0.67367287]
Recall: [0.92158627 0.82823529 0.82015069 0.70542481 0.82265   ]
F1 score: [0.95305831 0.74968137 0.7274334  0.6809125  0.74074519]
Accuracy:0.8988338451317393
Confusion Matrix[[647301  40845  13654    577]
 [  7420 112640  15358    582]
 [  1152  10534  56387    679]
 [   116    482    879   3537]]
68.4737478799521 82.82352941176471 74.96813654530268 65.35501518347667 82.01506865254828 72.74333999870993 65.80465116279069 70.54248105305146 68.09125036095871 67.36728686649437 82.26500004767216 74.07451923076923
