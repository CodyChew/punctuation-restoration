Namespace(alpha_del=0.4, alpha_sub=0.4, augment_rate=0.15, augment_type='all', batch_size=8, cuda=True, data_path='data', decay=0, epoch=15, freeze_bert=False, gradient_clip=-1, language='ted_talk_with_utt', lr=5e-06, lstm_dim=-1, name='punctuation-restore', pretrained_model='xlm-roberta-base', save_path='out/fullTedTalk_newUTT_triangle0To1_seed1', seed=1, sequence_length=256, sliding_window=True, stride_size=0.5, sub_style='unk', trained_model_path=False)
epoch: 0, Train loss: 0.34564603226827256, Train accuracy: 0.8812842582911049
epoch: 0, Val loss: 0.28599444751350833, Val accuracy: 0.8896360452119234
val_Precision: [0.9556885  0.72883185 0.58696199        nan 0.67061363]
val_Recall: [0.95393456 0.65110219 0.764889   0.         0.67472725]
val_F1: [0.95481073 0.68777782 0.66421635        nan 0.67266415]

epoch: 1, Train loss: 0.21843716724129286, Train accuracy: 0.9168803211434117
epoch: 1, Val loss: 0.2549837726083513, Val accuracy: 0.9026505121507542
val_Precision: [0.96255084 0.74422289 0.65124081 0.57096248 0.70850832]
val_Recall: [0.95567405 0.72210642 0.74741269 0.47597461 0.72542675]
val_F1: [0.95910012 0.73299786 0.69602034 0.51915946 0.71686773]

epoch: 2, Train loss: 0.1932023504169553, Train accuracy: 0.9240595953030111
epoch: 2, Val loss: 0.2447523981937602, Val accuracy: 0.9089764356857799
val_Precision: [0.96085123 0.75758142 0.69197992 0.64787043 0.73413676]
val_Recall: [0.96269673 0.73946011 0.72283662 0.48957389 0.72942388]
val_F1: [0.9617731  0.74841109 0.70707179 0.5577072  0.73177273]

epoch: 3, Train loss: 0.18026794216553446, Train accuracy: 0.9276982430711882
epoch: 3, Val loss: 0.24249279344264354, Val accuracy: 0.9107536715261714
val_Precision: [0.9647039  0.76181464 0.68392848 0.63307494 0.73307785]
val_Recall: [0.96140764 0.74472354 0.74580159 0.55530372 0.74144987]
val_F1: [0.96305295 0.75317215 0.71352622 0.59164453 0.73724009]

epoch: 4, Train loss: 0.1722421089952886, Train accuracy: 0.9300660873125508
epoch: 4, Val loss: 0.24259475961188162, Val accuracy: 0.9130808144203163
val_Precision: [0.95906551 0.80224225 0.67624283 0.66231648 0.752837  ]
val_Recall: [0.96817405 0.71318203 0.7726168  0.55213055 0.72893938]
val_F1: [0.96359826 0.75509514 0.72122456 0.60222497 0.74069548]

epoch: 5, Train loss: 0.16595308309292026, Train accuracy: 0.932154848402841
epoch: 5, Val loss: 0.2412612757454394, Val accuracy: 0.9135490323827512
val_Precision: [0.95645674 0.8195965  0.67736934 0.59252898 0.76054583]
val_Recall: [0.9704649  0.70127417 0.77540209 0.62556664 0.72331571]
val_F1: [0.9634099  0.75583265 0.7230781  0.60859978 0.74146372]

epoch: 6, Train loss: 0.16103135136314461, Train accuracy: 0.9337113988564062
epoch: 6, Val loss: 0.2402144085821529, Val accuracy: 0.9152485639655471
val_Precision: [0.96183403 0.77984431 0.71407024 0.64128352 0.75577255]
val_Recall: [0.96709981 0.75440356 0.7239562  0.60698096 0.74194302]
val_F1: [0.96445973 0.76691301 0.71897924 0.62366092 0.74879394]

epoch: 7, Train loss: 0.1570600244806107, Train accuracy: 0.9350500531053391
epoch: 7, Val loss: 0.24346192290187638, Val accuracy: 0.9150393601950975
val_Precision: [0.96097399 0.81218805 0.6732242  0.64282285 0.75640855]
val_Recall: [0.96814299 0.71867997 0.78495945 0.60698096 0.73754791]
val_F1: [0.96454517 0.76257819 0.72481089 0.62438797 0.74685918]

epoch: 8, Train loss: 0.15355035183000548, Train accuracy: 0.9362748178938693
epoch: 8, Val loss: 0.23610722193069075, Val accuracy: 0.915370101394094
val_Precision: [0.96535149 0.78462147 0.69245634 0.61489632 0.74909266]
val_Recall: [0.96431197 0.74899682 0.76325059 0.6586582  0.75178876]
val_F1: [0.96483145 0.76639538 0.72613202 0.63602539 0.75043829]

epoch: 9, Train loss: 0.15043032509030269, Train accuracy: 0.9373277802503563
epoch: 9, Val loss: 0.24323254548024884, Val accuracy: 0.9153820558952627
val_Precision: [0.96379977 0.77302658 0.71849057 0.61512605 0.75212968]
val_Recall: [0.96573825 0.75862473 0.72788837 0.66364461 0.74707353]
val_F1: [0.96476804 0.76575795 0.72315894 0.63846489 0.74959308]

epoch: 10, Train loss: 0.14779682535526056, Train accuracy: 0.9382887220711824
epoch: 10, Val loss: 0.23496689573573445, Val accuracy: 0.9166990434406649
val_Precision: [0.95913772 0.80086391 0.71303615 0.63174046 0.76737968]
val_Recall: [0.97032253 0.73914743 0.73902952 0.6532185  0.73747004]
val_F1: [0.96469771 0.76876901 0.72580018 0.64229998 0.75212763]

epoch: 11, Train loss: 0.14493679339713694, Train accuracy: 0.9393728911403972
epoch: 11, Val loss: 0.24478588990626102, Val accuracy: 0.9167169751924177
val_Precision: [0.96329303 0.79818457 0.69503265 0.62985596 0.758222  ]
val_Recall: [0.96725771 0.74121893 0.76721007 0.65412511 0.74779163]
val_F1: [0.9652713  0.76864774 0.72934    0.64176117 0.75297069]

epoch: 12, Train loss: 0.1427273055671415, Train accuracy: 0.9401868482790122
epoch: 12, Val loss: 0.24020946218460765, Val accuracy: 0.9166771268551891
val_Precision: [0.96441832 0.78046729 0.71398653 0.63785461 0.75583663]
val_Recall: [0.9661783  0.76159518 0.73545234 0.65231188 0.75122639]
val_F1: [0.96529751 0.77091576 0.72456048 0.64500224 0.75352446]

epoch: 13, Train loss: 0.1403038377135645, Train accuracy: 0.941056359302405
epoch: 13, Val loss: 0.2419826601220704, Val accuracy: 0.9158642207757276
val_Precision: [0.96627019 0.80368019 0.66772916 0.62543253 0.74881166]
val_Recall: [0.96437927 0.73404034 0.80085197 0.65548504 0.75370946]
val_F1: [0.96532381 0.76728335 0.72825696 0.64010624 0.75125258]

epoch: 14, Train loss: 0.13823994531952083, Train accuracy: 0.9418933510688166
epoch: 14, Val loss: 0.2502256570170284, Val accuracy: 0.9164758927521852
val_Precision: [0.9629117  0.80150052 0.69069288 0.64025501 0.75830919]
val_Recall: [0.96708169 0.73626817 0.77714972 0.63735267 0.74733309]
val_F1: [0.96499219 0.76750076 0.73137512 0.63880055 0.75278113]

Precision: [0.96598177 0.80741581 0.71316812 0.71235195 0.77160559]
Recall: [0.96700775 0.76966912 0.77434838 0.67171919 0.76886149]
F1 score: [0.96649449 0.78809074 0.7425001  0.69143913 0.7702311 ]
Accuracy:0.9214399496570165
Confusion Matrix[[679204  15130   7784    259]
 [ 18072 104675  12748    505]
 [  5397   9521  53238    596]
 [   450    316    880   3368]]
80.74158066058838 76.96691176470588 78.80907386633137 71.31681178834562 77.43483825925064 74.25001046010516 71.2351945854484 67.17191862784205 69.14391295421885 77.16055879820112 76.88614932829914 77.02310965505056

